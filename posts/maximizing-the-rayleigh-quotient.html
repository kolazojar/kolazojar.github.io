<!doctype html>
<html lang="en">
    <head>
        <title>Matt Krol - Maximizing the Rayleigh Quotient</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta charset="utf-8" />
        <link rel="stylesheet" type="text/css" href="/theme/style.css">
        <link href="https://mattkrol.me/feeds/atom.xml" type="application/atom+xml" rel="alternate" title="Matt Krol - Atom Feed" />

<script>window.MathJax = { tex: { tags: 'ams' } };</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body>
        <div class="box">
            <header>
                <h1 id="sitename">Matt Krol</h1>
                <p>Electrical Engineer, Bassist, Composer</p>
            </header>
            <nav><p>
                <a href="https://mattkrol.me/">Blog</a>&nbsp;
                <a href="https://mattkrol.me/about.html">About</a>&nbsp;
                <a href="https://mattkrol.me/links.html">Links</a>&nbsp;
                <a href="https://mattkrol.me/resume.html">Resume</a>&nbsp;
            </p></nav>
            <hr>
<h1>Maximizing the Rayleigh Quotient</h1>
<time datetime="2021-01-30T00:00:00-05:00">
    Date: Sat 30 January 2021
</time>
<div>
    Categories: <a href="https://mattkrol.me/category/math.html">Math</a>
</div>
<div>
    Tags:
    <a href="https://mattkrol.me/tag/optimization.html">Optimization</a>
    <a href="https://mattkrol.me/tag/proofs.html">Proofs</a>
</div>
<div>
    <p>The rayleigh quotient maximization problem naturally occurs in many disciplines&mdash;machine learning, statistics, quantum mechanics, and portfolio optimization just to name a few. Although the solution of this problem is widely known, many texts state the solution without offering any proof. In this post, I will present proof of the rayleigh quotient maximization problem solution that will hopefully satisfy the most inquisitive of readers!</p>
<p>We will begin by stating the rayleigh quotient maximization problem.</p>
<p>Let <span class="math">\(A,B\in\mathbb{R}^{d\times d}\)</span> be two symmetric positive semi-definite matrices and <span class="math">\(x\in\mathbb{R}^{d}\)</span> be a column vector. Additionally, we will require <span class="math">\(B\)</span> to be invertible. We will show that the rayleigh quotient problem in <span class="math">\((\ref{problem-statement})\)</span> is maximized whenever <span class="math">\(x\)</span> is collinear to the eigenvector of <span class="math">\(B^{-1}A\)</span> that corresponds to the largest eigenvalue.</p>
<div class="math">\begin{equation}
    \label{problem-statement}
    \mathop{\arg\max}\limits_{x}\frac{x^TAx}{x^TBx}
\end{equation}</div>
<p>Since <span class="math">\(B\)</span> is symmetric, we know that there exists an orthonormal matrix <span class="math">\(P\in\mathbb{R}^{d\times d}\)</span> and a diagonal matrix <span class="math">\(D\in\mathbb{R}^{d\times d}\)</span> such that <span class="math">\(B=PDP^T\)</span>. Furthermore, since <span class="math">\(B\)</span> is also invertible and positive semi-definite, we know that the diagonal entries of <span class="math">\(D\)</span>, i.e., the eigenvalues of <span class="math">\(B\)</span>, are positive and non-zero. Consequently, the matrix <span class="math">\(B^{-\frac{1}{2}}\)</span> exists, and we can use the invertible substitution in <span class="math">\((\ref{sub})\)</span> to transform our optimization problem in <span class="math">\((\ref{problem-statement})\)</span> to the equivalent optimization problem shown in <span class="math">\((\ref{sub-problem})\)</span>.</p>
<div class="math">\begin{equation}
    \label{sub}
    x=B^{-\frac{1}{2}}w
\end{equation}</div>
<div class="math">\begin{equation}
    \label{sub-problem}
    \mathop{\arg\max}\limits_{w}\frac{w^TB^{-\frac{1}{2}}AB^{-\frac{1}{2}}w}{w^Tw}
\end{equation}</div>
<p>Considering the fact that <span class="math">\(w=\lVert w\rVert\frac{w}{\lVert w\rVert}\)</span>, we notice that <span class="math">\((\ref{sub-problem})\)</span> is invariant to the norm of <span class="math">\(w\)</span>. So the optimization problem in <span class="math">\((\ref{sub-sub-problem})\)</span> is equivalent to the optimization problem in <span class="math">\((\ref{sub-problem})\)</span>. In order to see this, suppose that <span class="math">\(w^*\)</span> is a solution to <span class="math">\((\ref{sub-sub-problem})\)</span>, then any <span class="math">\(cw^*\)</span> with <span class="math">\(c\in\mathbb{R}\)</span> is a solution to <span class="math">\((\ref{sub-problem})\)</span>.</p>
<div class="math">\begin{equation}
    \label{sub-sub-problem}
    \mathop{\arg\max}\limits_{w,\lVert w \rVert=1}\frac{w^TB^{-\frac{1}{2}}AB^{-\frac{1}{2}}w}{w^Tw}=\mathop{\arg\max}\limits_{w,\lVert w\rVert=1}w^TB^{-\frac{1}{2}}AB^{-\frac{1}{2}}w
\end{equation}</div>
<p>Since the matrices <span class="math">\(B^{-\frac{1}{2}}\)</span> and <span class="math">\(A\)</span> are symmetric, the product <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}\)</span> is also clearly symmetric. Hence, <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}\)</span> can be orthogonally diagonalized by an orthonormal matrix <span class="math">\(V\)</span> and a diagonal matrix <span class="math">\(\Sigma\)</span> such that <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}=V\Sigma V^T\)</span>. </p>
<div class="math">\begin{equation}
    \mathop{\arg\max}\limits_{w,\lVert w\rVert=1}w^TB^{-\frac{1}{2}}AB^{-\frac{1}{2}}w=\mathop{\arg\max}\limits_{w,\lVert w\rVert=1}w^TV\Sigma V^Tw
\end{equation}</div>
<p>Let <span class="math">\(\lambda_i\in\mathbb{R}\)</span> and <span class="math">\(v_i\in\mathbb{R^{d}}\)</span> with <span class="math">\(i\in\{1,2,\dots,d\}\)</span> be the eigenvalues and eigenvectors of <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}\)</span> respectively. We note the following about the matrices <span class="math">\(\Sigma\)</span> and <span class="math">\(V\)</span>.</p>
<div class="math">\begin{align}
    V&amp;=\begin{bmatrix}v_1 &amp; v_2 &amp; \dots &amp; v_d\end{bmatrix}\\
    \Sigma&amp;=\operatorname{diag}\{\lambda_1,\lambda_2,\dots,\lambda_d\}
\end{align}</div>
<p>Now we can rewrite <span class="math">\(w^TV\Sigma V^Tw\)</span> as a summation and rewrite the optimization problem in <span class="math">\((\ref{sub-sub-problem})\)</span> as the one shown in <span class="math">\((\ref{summation})\)</span>.</p>
<div class="math">\begin{align}
    \mathop{\arg\max}\limits_{w,\lVert w\rVert=1}w^TV\Sigma V^Tw&amp;=\mathop{\arg\max}\limits_{w,\lVert w\rVert=1}\begin{bmatrix}w^Tv_1 &amp; w^Tv_2 &amp; \dots &amp; w^Tv_d\end{bmatrix}\Sigma\begin{bmatrix}w^Tv_1\\w^Tv_2\\\vdots\\w^Tv_d\end{bmatrix}\\
    &amp;=\mathop{\arg\max}\limits_{w,\lVert w\rVert=1}\sum_{i=1}^{d}\left(w^Tv_i\right)^2\lambda_i\label{summation}
\end{align}</div>
<p>Since <span class="math">\(V\)</span> is orthonormal, we have <span class="math">\(w^TVV^Tw=1\)</span>, so the summation <span class="math">\(\sum_{i=1}^{d}\left(w^Tv_i\right)^2=1\)</span> for all <span class="math">\(w\)</span>. Additionally, since <span class="math">\(A\)</span> and <span class="math">\(B^{-\frac{1}{2}}\)</span> are symmetric and positive semi-definite, <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}\)</span> is also symmetric and positive semi-definite, but not necessarily invertible. Thus, we conclude that all <span class="math">\(\lambda_i\ge 0\)</span>. Now suppose that <span class="math">\(\lambda^*=\lambda_d\)</span> is the largest eigenvalue of <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}\)</span>. So <span class="math">\((\ref{ortho})\)</span> must be true.</p>
<div class="math">\begin{equation}
    \label{ortho}
    \frac{\lambda_1}{\lambda^*}\left(w^Tv_1\right)^2+\frac{\lambda_2}{\lambda^*}\left(w^Tv_2\right)^2+\dots+\frac{\lambda_{d-1}}{\lambda^*}\left(w^Tv_{d-1}\right)^2+\left(w^Tv_d\right)\leq 1
\end{equation}</div>
<p>It follows from <span class="math">\((\ref{ortho})\)</span> that <span class="math">\((\ref{almost-done})\)</span> must also be true.
</p>
<div class="math">\begin{equation}
    \label{almost-done}
    \sum_{i=1}^d\left(w^Tv_i\right)^2\lambda_i\leq \lambda^*
\end{equation}</div>
<p>Clearly, <span class="math">\(\sum_{i=1}^d\left(w^Tv_i\right)^2\lambda_i=\lambda^*\)</span> if and only if <span class="math">\(w\)</span> is collinear to the eigenvector of <span class="math">\(B^{-\frac{1}{2}}AB^{-\frac{1}{2}}\)</span> that corresponds to the largest eigenvalue. Hence, under these conditions, we have maximized <span class="math">\((\ref{sub-problem})\)</span>. Using the substitution in <span class="math">\((\ref{sub})\)</span>, a solution to <span class="math">\((\ref{sub-problem})\)</span> can be translated into a solution to <span class="math">\((\ref{problem-statement})\)</span>.</p>
<div class="math">\begin{align}
    B^{-\frac{1}{2}}AB^{-\frac{1}{2}}w&amp;=\lambda w\\
    B^{-\frac{1}{2}}AB^{-\frac{1}{2}}B^{\frac{1}{2}}x&amp;=\lambda B^{\frac{1}{2}}x\\
    B^{-1}Ax&amp;=\lambda x\\
\end{align}</div>
<p>Thus, <span class="math">\((\ref{problem-statement})\)</span> is maximized when <span class="math">\(x\)</span> is collinear to the eigenvector of <span class="math">\(B^{-1}A\)</span> that corresponds to the largest eigenvector. This completes the proof!</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
            <hr>
            <footer>
                <p>&copy; Copyright 2021 Matt Krol. Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.</p>
            </footer>
        </div>
    </body>
</html>