<!doctype html>
<html lang="en">
    <head>
        <title>Matt Krol - Maximizing the Rayleigh&nbsp;Quotient</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="M-zX01scDIcAKac84ZX6SNHuf5qUtsQivPka4GreRac" />
        <meta charset="utf-8" />
        <link rel="stylesheet" type="text/css" href="/theme/style.css">
        <link href="https://mattkrol.me/feeds/atom.xml" type="application/atom+xml" rel="alternate" title="Matt Krol - Atom Feed" />

<script>window.MathJax = { tex: { tags: 'ams' } };</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body>
        <div class="box">
            <header>
                <h1 id="sitename">Matt Krol</h1>
                <p>Electrical Engineer, Bassist, Composer</p>
            </header>
            <nav><p>
                <a href="https://mattkrol.me/">Blog</a>&nbsp;
                <a href="https://mattkrol.me/about.html">About</a>&nbsp;
                <a href="https://mattkrol.me/links.html">Links</a>&nbsp;
                <a href="https://mattkrol.me/resume.html">Resume</a>&nbsp;
            </p></nav>
            <hr>
<h1>Maximizing the Rayleigh&nbsp;Quotient</h1>
<time datetime="2021-01-30T00:00:00-05:00">
    Date: Sat 30 January 2021
</time>
<div>
    Categories: <a href="https://mattkrol.me/category/math.html">Math</a>
</div>
<div>
    Tags:
    <a href="https://mattkrol.me/tag/optimization.html">Optimization</a>
    <a href="https://mattkrol.me/tag/proofs.html">Proofs</a>
</div>
<div>
    <p>The rayleigh quotient maximization problem naturally occurs in many disciplines&mdash;machine learning, statistics, quantum mechanics, and portfolio optimization just to name a few. Although the solution of this problem is widely known, many texts state the solution without offering any proof. In this post, I will present proof of the rayleigh quotient maximization problem solution that will hopefully satisfy the most inquisitive of&nbsp;readers!</p>
<p>The following proof will use only basic facts from linear algebra. However, one could argue that using lagrange multipliers would provide a more concise proof. Although this may be true, I hope that readers will still find this proof&nbsp;helpful.</p>
<p>Let $A,B\in\mathbb{R}^{d\times d}$ be two symmetric positive semi-definite matrices and $x\in\mathbb{R}^{d}$ be a column vector. Additionally, we will require $B$ to be invertible. We will begin the proof by stating the rayleigh quotient maximization problem
\begin{equation}
    \label{problem-statement}
    \mathop{\max}\limits_{x}\frac{x^TAx}{x^TBx}.
\end{equation}
We will show that the rayleigh quotient problem in (\ref{problem-statement}) is maximized whenever $x$ is collinear to the eigenvector of $B^{-1}A$ that corresponds its largest&nbsp;eigenvalue.</p>
<p>Since $B$ is symmetric, we know that there exists an orthonormal matrix $P\in\mathbb{R}^{d\times d}$ and a diagonal matrix $D\in\mathbb{R}^{d\times d}$ such that $B=<span class="caps">PDP</span>^T$. Furthermore, since $B$ is also invertible and positive semi-definite, we know that the diagonal entries of $D$, i.e., the eigenvalues of $B$, are positive and non-zero. Consequently, the matrix $B^{-\frac{1}{2}}$ exists, and we can use the invertible substitution 
\begin{equation}
    \label{sub}
    x=B^{-\frac{1}{2}}w
\end{equation}
to yield
\begin{equation}
    \label{sub-problem}
    \mathop{\max}\limits_{x}\frac{x^TAx}{x^TBx}=\mathop{\max}\limits_{w}\frac{w^<span class="caps">TB</span>^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}w}{w^Tw}.
\end{equation}
Now we can observe that (\ref{sub-problem}) is invariant to the norm of $w$, so
\begin{equation}
    \label{sub-sub-problem}
    \mathop{\max}\limits_{w}\frac{w^<span class="caps">TB</span>^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}w}{w^Tw}=\mathop{\max}\limits_{\lVert w\rVert_2=1}w^<span class="caps">TB</span>^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}w.&nbsp;\end{equation}</p>
<p>Since the matrices $B^{-\frac{1}{2}}$ and $A$ are symmetric, the product $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ is also clearly symmetric. Hence, $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ can be orthogonally diagonalized by an orthonormal matrix $V$ and a diagonal matrix $\Sigma$ such that $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}=V\Sigma V^T$, and
\begin{equation}
    \mathop{\max}\limits_{\lVert w\rVert_2=1}w^<span class="caps">TB</span>^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}w=\mathop{\max}\limits_{\lVert w\rVert_2=1}w^<span class="caps">TV</span>\Sigma V^Tw.
\end{equation}
Let $\lambda_i\in\mathbb{R}$ and $v_i\in\mathbb{R^{d}}$ with $i\in{1,2,\dots,d}$ be the eigenvalues and eigenvectors of $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ respectively with
\begin{align}
    V&amp;=\begin{bmatrix}v_1 <span class="amp">&amp;</span> v_2 <span class="amp">&amp;</span> \dots <span class="amp">&amp;</span> v_d\end{bmatrix}\
    \Sigma&amp;=\operatorname{diag}{\lambda_1,\lambda_2,\dots,\lambda_d}.
\end{align}
Now we can rewrite $w^<span class="caps">TV</span>\Sigma V^Tw$ as a summation, so
\begin{equation}
    \mathop{\max}\limits_{\lVert w\rVert_2=1}w^<span class="caps">TV</span>\Sigma V^Tw=\mathop{\max}\limits_{\lVert w\rVert_2=1}\sum_{i=1}^{d}\left(w^Tv_i\right)^2\lambda_i\label{summation}.
\end{equation}
Since $V$ is orthonormal, we have $w^<span class="caps">TVV</span>^Tw=1$, so the summation $\sum_{i=1}^{d}\left(w^Tv_i\right)^2=1$ for all $w$ with $\lVert w\rVert_2=1$. Additionally, since $A$ and $B^{-\frac{1}{2}}$ are symmetric and positive semi-definite, $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ is also symmetric and positive semi-definite, but not necessarily invertible. Thus, we conclude that all $\lambda_i\ge 0$. Since there will be at least one non-zero eigenvalue, suppose that $\lambda^<em>=\lambda_d$ is the largest eigenvalue of $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$, then
\begin{equation}
    \label{ortho}
    \frac{\lambda_1}{\lambda^</em>}\left(w^Tv_1\right)^2+\frac{\lambda_2}{\lambda^<em>}\left(w^Tv_2\right)^2+\dots+\frac{\lambda_{d-1}}{\lambda^</em>}\left(w^Tv_{d-1}\right)^2+\left(w^Tv_d\right)^2\leq 1.
\end{equation}
It follows from (\ref{ortho}) that
\begin{equation}
    \label{almost-done}
    \sum_{i=1}^d\lambda_i\left(w^Tv_i\right)^2\leq \lambda^*.&nbsp;\end{equation}</p>
<p>Clearly, $\sum_{i=1}^d\left(w^Tv_i\right)^2\lambda_i=\lambda^*$ if and only if $w$ is collinear to the eigenvector of $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ that corresponds to the largest eigenvalue. Hence, under these conditions, we have maximized&nbsp;(\ref{sub-problem}).</p>
<p>Using the substitution in (\ref{sub}), a solution to (\ref{sub-problem}) can be translated into a solution to (\ref{problem-statement}). Suppose that $w$ and $\lambda$ are set to be any eigenpair of $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$, then
\begin{align}
    B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}w&amp;=\lambda w\
    B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}B^{\frac{1}{2}}x&amp;=\lambda B^{\frac{1}{2}}x\
    B^{-1}Ax&amp;=\lambda x.
\end{align}
We have just shown that any eigenvalue of $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ is also an eigenvalue of $B^{-1}A$. Since the later two matrices have the same rank, which is simply equal to the rank of $A$, we can conclude that they have the same set of eigenvalues. Accordingly, (\ref{sub}) gives us a one-to-one correspondence between their eigenvectors. Thus, the maximum eigenvalue of $B^{-\frac{1}{2}}<span class="caps">AB</span>^{-\frac{1}{2}}$ is also the maximum eigenvalue of $B^{-1}A$. So if $w^<em>$ is a solution to (\ref{sub-problem}), then $x^</em>=B^{-\frac{1}{2}}w^*$ is a solution to (\ref{problem-statement}). Hence, (\ref{problem-statement}) is maximized when $x$ is collinear to the eigenvector of $B^{-1}A$ that corresponds to its largest eigenvector. This completes the&nbsp;proof!</p>
</div>
            <hr>
            <footer>
                <p>&copy; Copyright 2021 Matt Krol. This website is powered by <a href="https://getpelican.com/">Pelican</a> and hosted via <a href="https://pages.github.com/">GitHub Pages</a>.</p>
            </footer>
        </div>
    </body>
</html>